{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1263952d",
   "metadata": {},
   "source": [
    "# Binned Training Notebook (Training-Only)\n",
    "\n",
    "Train one model per cluster_Et bin using separate input files per bin. After training, compute per-bin AUC and the correlation between isoET and the model BDT score (predicted probability).\n",
    "\n",
    "Bins: [5, 15, 25, 40] (GeV) — models trained on [5–15), [15–25), [25–40).\n",
    "\n",
    "Outputs per bin:\n",
    "- Trained model artifact (joblib)\n",
    "- Metadata JSON\n",
    "- Validation AUC\n",
    "- isoET vs BDT score correlation (Pearson/Spearman)\n",
    "- Plots: AUC by bin, isoET–BDT hexbin/scatter, optional ROC curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cbb4e346",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Section 1: Import Libraries and Set Seed\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import roc_auc_score, roc_curve\n",
    "from sklearn.ensemble import HistGradientBoostingClassifier\n",
    "\n",
    "# Optional: XGBoost (preferred if available)\n",
    "try:\n",
    "    import xgboost as xgb\n",
    "    XGB_AVAILABLE = True\n",
    "except Exception:\n",
    "    xgb = None\n",
    "    XGB_AVAILABLE = False\n",
    "\n",
    "from scipy.interpolate import UnivariateSpline\n",
    "from scipy import stats\n",
    "import joblib\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Reproducibility\n",
    "GLOBAL_SEED = 42\n",
    "np.random.seed(GLOBAL_SEED)\n",
    "\n",
    "# Matplotlib defaults\n",
    "plt.rcParams[\"figure.figsize\"] = (7.5, 5)\n",
    "plt.rcParams[\"axes.grid\"] = True\n",
    "sns.set_context(\"talk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d94f1d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Section 2: Configure cluster_Et Bins and Per-Bin Input Files\n",
    "\n",
    "# Reweighting configuration\n",
    "# if CLASS_REWEIGHT: make the weighted signal and background the same\n",
    "CLASS_REWEIGHT = True\n",
    "\n",
    "# --- NEW: Option to train a single model across all Et bins ---\n",
    "# If True, one model is trained on all data. Plots are still per-bin.\n",
    "# If False, one model is trained for each bin.\n",
    "TRAIN_SINGLE_MODEL = False\n",
    "\n",
    "# Define bin edges and derive labels for evaluation\n",
    "BIN_EDGES = [5, 15, 25, 40]\n",
    "BIN_LABELS = [f\"{BIN_EDGES[i]}_{BIN_EDGES[i+1]}\" for i in range(len(BIN_EDGES)-1)]\n",
    "\n",
    "# --- Input File Configuration ---\n",
    "# Option 1: Use the same files for all bins (current setup)\n",
    "# The data loader will filter events by pT for each bin.\n",
    "# This is useful when you have inclusive data files.\n",
    "USE_SINGLE_FILE_SET = False\n",
    "\n",
    "SINGLE_FILE_SET = {\n",
    "    \"signal\": \"shapes_photon20.txt\",\n",
    "    \"background\": \"shapes_jet30.txt\"\n",
    "}\n",
    "\n",
    "# Option 2: Use different files for each pT bin.\n",
    "# This is useful if your data is already split into separate files by pT.\n",
    "# To use this, set USE_SINGLE_FILE_SET = False and edit the paths below.\n",
    "PER_BIN_FILE_SETS = {\n",
    "    \"5_15\": {\n",
    "        \"signal\": \"shapes_photon5.txt\",      # Example path\n",
    "        \"background\": \"shapes_jet15.txt\"     # Example path\n",
    "    },\n",
    "    \"15_25\": {\n",
    "        \"signal\": \"shapes_photon10.txt\",     # Example path\n",
    "        \"background\": \"shapes_jet20.txt\"    # Example path\n",
    "    },\n",
    "    \"25_40\": {\n",
    "        \"signal\": \"shapes_photon20.txt\",     # Example path\n",
    "        \"background\": \"shapes_jet30.txt\"    # Example path\n",
    "    },\n",
    "}\n",
    "\n",
    "# Logic to select the configuration\n",
    "if USE_SINGLE_FILE_SET:\n",
    "    PER_BIN_FILES = {bin_label: SINGLE_FILE_SET for bin_label in BIN_LABELS}\n",
    "else:\n",
    "    PER_BIN_FILES = PER_BIN_FILE_SETS\n",
    "\n",
    "\n",
    "# Column names from model_training.ipynb\n",
    "COLUMNS = [\n",
    "    \"cluster_Et\", \"cluster_Eta\", \"cluster_Phi\", \"vertexz\",\n",
    "    \"e11_over_e33\", \"e32_over_e35\", \"e11_over_e22\", \"e11_over_e13\",\n",
    "    \"e11_over_e15\", \"e11_over_e17\", \"e11_over_e31\",\n",
    "    \"e11_over_e51\", \"e11_over_e71\", \"e22_over_e33\",\n",
    "    \"e22_over_e35\", \"e22_over_e37\", \"e22_over_e53\",\n",
    "    \"cluster_prob\", \"cluster_weta_cogx\", \"cluster_wphi_cogx\",\n",
    "    \"cluster_et1\", \"cluster_et2\", \"cluster_et3\", \"cluster_et4\",\n",
    "    \"cluster_w32\", \"cluster_w52\", \"cluster_w72\", \n",
    "    \"recoisoET\", \"is_tight\", \"pid\"\n",
    "]\n",
    "\n",
    "# Key column names\n",
    "PT_COL = \"cluster_Et\"\n",
    "ISO_COL = \"recoisoET\"      # Updated from \"isoET\"\n",
    "LABEL_COL = \"label\"\n",
    "\n",
    "# Feature columns to train on from model_training.ipynb\n",
    "FEATURES = [\n",
    "    \"vertexz\",\n",
    "    \"cluster_Eta\",\n",
    "    \"e11_over_e33\",\n",
    "    \"cluster_et1\",\n",
    "    \"cluster_et2\",\n",
    "    \"cluster_et3\",\n",
    "    \"cluster_et4\",\n",
    "]\n",
    "\n",
    "# Output directory for models/metrics\n",
    "OUT_DIR = Path(\"binned_models\")\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Train/val split\n",
    "TRAIN_SIZE = 0.8\n",
    "VAL_SIZE = 1 - TRAIN_SIZE\n",
    "STRATIFY = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c83635ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Section 3: Load and Concatenate Data for Each Bin\n",
    "try:\n",
    "    import uproot  # for ROOT files\n",
    "except Exception:\n",
    "    uproot = None\n",
    "\n",
    "\n",
    "def load_single(path: str, names: list) -> pd.DataFrame:\n",
    "    ext = os.path.splitext(path)[1].lower()\n",
    "    if ext in [\".csv\", \".txt\"]:\n",
    "        return pd.read_csv(path, sep=r\"\\s+\", header=0, names=names)\n",
    "    if ext in [\".pq\", \".parquet\"]:\n",
    "        return pd.read_parquet(path)\n",
    "    if ext in [\".root\"]:\n",
    "        if uproot is None:\n",
    "            raise ImportError(\"uproot is required to read ROOT files. pip install uproot\")\n",
    "        # Heuristic: read first tree and all branches\n",
    "        with uproot.open(path) as f:\n",
    "            # Pick the first TTree-like object\n",
    "            trees = [k for k in f.keys() if isinstance(f[k], uproot.behaviors.TTree.TTree)]\n",
    "            if not trees:\n",
    "                # Fall back: search members\n",
    "                trees = [k for k, v in f.items() if hasattr(v, \"arrays\")]\n",
    "            key = trees[0] if trees else list(f.keys())[0]\n",
    "            arrs = f[key].arrays(library=\"pd\")\n",
    "            return arrs\n",
    "    raise ValueError(f\"Unsupported file type: {ext}\")\n",
    "\n",
    "\n",
    "def load_bin_data(bin_label: str, et_min: float, et_max: float) -> pd.DataFrame:\n",
    "    file_map = PER_BIN_FILES.get(bin_label, {})\n",
    "    if not file_map:\n",
    "        raise FileNotFoundError(f\"No input files configured for bin {bin_label}\")\n",
    "\n",
    "    dfs = []\n",
    "    for file_type, path in file_map.items():\n",
    "        if not os.path.exists(path):\n",
    "            raise FileNotFoundError(f\"Configured file not found: {path}\")\n",
    "        df_i = load_single(path, names=COLUMNS)\n",
    "        if file_type == \"signal\":\n",
    "            df_i[LABEL_COL] = 1\n",
    "            df_i = df_i[df_i[\"pid\"].isin([1,2])] # photon only\n",
    "        elif file_type == \"background\":\n",
    "            df_i[LABEL_COL] = 0\n",
    "            df_i = df_i[~df_i[\"pid\"].isin([1,2])] # reject electrons\n",
    "        dfs.append(df_i)\n",
    "    df = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "    # Validate required columns\n",
    "    base_required = {PT_COL, ISO_COL, LABEL_COL}\n",
    "    if FEATURES:\n",
    "        required = base_required.union(FEATURES)\n",
    "    else:\n",
    "        required = base_required\n",
    "    missing = sorted(required - set(df.columns))\n",
    "    if missing:\n",
    "        raise KeyError(f\"Missing required columns for bin {bin_label}: {missing}\")\n",
    "\n",
    "    # Drop rows with missing label and key columns\n",
    "    df = df.dropna(subset=[LABEL_COL])\n",
    "\n",
    "    # Filter by Et bin\n",
    "    df = df[(df[PT_COL] >= et_min) & (df[PT_COL] < et_max)]\n",
    "\n",
    "    # Ensure numeric dtypes for key columns\n",
    "    for c in [PT_COL, ISO_COL, LABEL_COL]:\n",
    "        if c in df.columns:\n",
    "            df[c] = pd.to_numeric(df[c], errors=\"coerce\")\n",
    "\n",
    "    # Drop rows with NA on key columns after coercion\n",
    "    df = df.dropna(subset=[PT_COL, ISO_COL, LABEL_COL])\n",
    "\n",
    "    return df\n",
    "\n",
    "# Section 3.1: Auto-detect FEATURES if empty (numeric columns excluding PT/ISO/LABEL)\n",
    "\n",
    "def autodetect_features(df: pd.DataFrame) -> list:\n",
    "    numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "    exclude = {PT_COL, ISO_COL, LABEL_COL}\n",
    "    feats = [c for c in numeric_cols if c not in exclude]\n",
    "    return feats\n",
    "\n",
    "\n",
    "def split_train_val_with_features(df: pd.DataFrame, features: list):\n",
    "    X = df[features].copy()\n",
    "    y = df[LABEL_COL].astype(int).values\n",
    "    w = df[\"weight\"].to_numpy(dtype=np.float32)\n",
    "    \n",
    "    strat = y if STRATIFY else None\n",
    "    \n",
    "    X_train, X_val, y_train, y_val, w_train, w_val = train_test_split(\n",
    "        X, y, w, train_size=TRAIN_SIZE, random_state=GLOBAL_SEED, stratify=strat\n",
    "    )\n",
    "    return X_train, X_val, y_train, y_val, w_train, w_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "59096dc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Section 3.5: Kinematic Reweighting (Eta)\n",
    "\n",
    "def apply_eta_reweighting(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Applies inverse-PDF reweighting to flatten the cluster_Eta distribution\n",
    "    for signal and background separately.\n",
    "    \"\"\"\n",
    "    df_out = df.copy()\n",
    "    eta_min, eta_max = -0.7, 0.7\n",
    "    n_bins_eta = 20\n",
    "    bins_eta = np.linspace(eta_min, eta_max, n_bins_eta + 1)\n",
    "    \n",
    "    # --- Class Imbalance Reweighting ---\n",
    "    if CLASS_REWEIGHT:\n",
    "        n_sig = (df_out[LABEL_COL] == 1).sum()\n",
    "        n_bkg = (df_out[LABEL_COL] == 0).sum()\n",
    "\n",
    "        if n_sig > 0 and n_bkg > 0:\n",
    "            weight_sig = (n_sig + n_bkg) / (2.0 * n_sig)\n",
    "            weight_bkg = (n_sig + n_bkg) / (2.0 * n_bkg)\n",
    "            df_out['class_weight'] = np.where(df_out[LABEL_COL] == 1, weight_sig, weight_bkg)\n",
    "        else:\n",
    "            df_out['class_weight'] = 1.0\n",
    "        \n",
    "    df_out[\"weight\"] = df_out['class_weight']\n",
    "\n",
    "\n",
    "    for label in [0, 1]:\n",
    "        mask = df_out[LABEL_COL] == label\n",
    "        if not mask.any():\n",
    "            continue\n",
    "            \n",
    "        eta_vals = df_out.loc[mask, \"cluster_Eta\"].values\n",
    "\n",
    "        # Histogram for eta PDF\n",
    "        hist_eta, bin_edges_eta = np.histogram(eta_vals, bins=bins_eta, density=True)\n",
    "        x_centers_eta = 0.5 * (bin_edges_eta[:-1] + bin_edges_eta[1:])\n",
    "\n",
    "        # Fit spline to eta PDF\n",
    "        spline_eta = UnivariateSpline(x_centers_eta, hist_eta, s=0.0)\n",
    "        pdf_eta_vals = spline_eta(eta_vals)\n",
    "        pdf_eta_vals = np.clip(pdf_eta_vals, a_min=1e-3, a_max=None)\n",
    "\n",
    "        # Compute inverse PDF weights for eta\n",
    "        weights_eta = 1.0 / pdf_eta_vals\n",
    "        weights_eta /= np.mean(weights_eta)\n",
    "\n",
    "        # Combine with class imbalance weight\n",
    "        df_out.loc[mask, \"weight\"] *= weights_eta\n",
    "        \n",
    "    return df_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "036538cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Section 3.6: QA Plots for Reweighting\n",
    "\n",
    "def plot_reweighting_qa(df_before: pd.DataFrame, df_after: pd.DataFrame, bin_label: str):\n",
    "    \"\"\"\n",
    "    Plots the cluster_Eta distribution and class balance before and after reweighting.\n",
    "    \"\"\"\n",
    "    fig = plt.figure(figsize=(14, 10))\n",
    "    gs = fig.add_gridspec(2, 2)\n",
    "\n",
    "    # --- Eta Before Reweighting ---\n",
    "    ax1 = fig.add_subplot(gs[0, 0])\n",
    "    for label_val, name in zip([0, 1], [\"Background\", \"Signal\"]):\n",
    "        mask = df_before[LABEL_COL] == label_val\n",
    "        if mask.any():\n",
    "            ax1.hist(df_before.loc[mask, \"cluster_Eta\"], bins=40, range=(-0.7, 0.7), \n",
    "                     histtype='step', label=name, density=True)\n",
    "    ax1.set_title(f\"Bin {bin_label}: Eta Before Reweighting\")\n",
    "    ax1.set_xlabel(\"cluster_Eta\")\n",
    "    ax1.set_ylabel(\"Density\")\n",
    "    ax1.legend()\n",
    "    ax1.grid(True)\n",
    "\n",
    "    # --- Eta After Reweighting ---\n",
    "    ax2 = fig.add_subplot(gs[0, 1])\n",
    "    for label_val, name in zip([0, 1], [\"Background\", \"Signal\"]):\n",
    "        mask = df_after[LABEL_COL] == label_val\n",
    "        if mask.any():\n",
    "            ax2.hist(df_after.loc[mask, \"cluster_Eta\"], bins=40, range=(-0.7, 0.7),\n",
    "                     weights=df_after.loc[mask, \"weight\"],\n",
    "                     histtype='step', label=name, density=True)\n",
    "    ax2.set_title(f\"Bin {bin_label}: Eta After Reweighting\")\n",
    "    ax2.set_xlabel(\"cluster_Eta\")\n",
    "    ax2.legend()\n",
    "    ax2.grid(True)\n",
    "\n",
    "    # --- Class Balance Before ---\n",
    "    ax3 = fig.add_subplot(gs[1, 0])\n",
    "    counts_before = df_before[LABEL_COL].value_counts()\n",
    "    n_sig_before = counts_before.get(1, 0)\n",
    "    n_bkg_before = counts_before.get(0, 0)\n",
    "    ax3.bar([\"Background\", \"Signal\"], [n_bkg_before, n_sig_before], color=['blue', 'orange'])\n",
    "    ax3.set_title(\"Class Counts Before Reweighting\")\n",
    "    ax3.set_ylabel(\"Number of Events\")\n",
    "    ax3.grid(axis='y')\n",
    "    for i, v in enumerate([n_bkg_before, n_sig_before]):\n",
    "        ax3.text(i, v, str(v), ha='center', va='bottom')\n",
    "\n",
    "\n",
    "    # --- Class Balance After ---\n",
    "    ax4 = fig.add_subplot(gs[1, 1])\n",
    "    w_sig_after = df_after.loc[df_after[LABEL_COL] == 1, \"weight\"].sum()\n",
    "    w_bkg_after = df_after.loc[df_after[LABEL_COL] == 0, \"weight\"].sum()\n",
    "    ax4.bar([\"Background\", \"Signal\"], [w_bkg_after, w_sig_after], color=['blue', 'orange'])\n",
    "    ax4.set_title(\"Class Yield After Reweighting\")\n",
    "    ax4.set_ylabel(\"Sum of Weights\")\n",
    "    ax4.grid(axis='y')\n",
    "    for i, v in enumerate([w_bkg_after, w_sig_after]):\n",
    "        ax4.text(i, v, f\"{v:.1f}\", ha='center', va='bottom')\n",
    "\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "eb2be386",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Section 4: Train/Validation Split per Bin\n",
    "\n",
    "def split_train_val(df: pd.DataFrame):\n",
    "    X = df[FEATURES].copy()\n",
    "    y = df[LABEL_COL].astype(int).values\n",
    "    if STRATIFY:\n",
    "        strat = y\n",
    "    else:\n",
    "        strat = None\n",
    "    X_train, X_val, y_train, y_val = train_test_split(\n",
    "        X, y, train_size=TRAIN_SIZE, random_state=GLOBAL_SEED, stratify=strat\n",
    "    )\n",
    "    return X_train, X_val, y_train, y_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4a642def",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Section 5: Build Preprocessing and Model Pipeline\n",
    "\n",
    "MODEL_CONFIG = {\n",
    "    \"classifier\": \"xgb\" if 'XGB_AVAILABLE' in globals() and XGB_AVAILABLE else \"hgb\",\n",
    "    \"params\": {\n",
    "        # Defaults for XGBClassifier; if HGB used, some keys are ignored\n",
    "        \"n_estimators\": 500,\n",
    "        \"max_depth\": 5,\n",
    "        \"learning_rate\": 0.05,\n",
    "        \"subsample\": 0.8,\n",
    "        \"colsample_bytree\": 0.8,\n",
    "        \"reg_alpha\": 0.0,\n",
    "        \"reg_lambda\": 1.0,\n",
    "        \"random_state\": GLOBAL_SEED,\n",
    "        \"n_jobs\": 4,\n",
    "        \"tree_method\": \"hist\",\n",
    "        #\"scale_pos_weight\": 10.0\n",
    "    },\n",
    "    \"use_scaler\": False,\n",
    "}\n",
    "\n",
    "\n",
    "def build_pipeline():\n",
    "    steps = []\n",
    "    steps.append((\"imputer\", SimpleImputer(strategy=\"median\")))\n",
    "    if MODEL_CONFIG.get(\"use_scaler\", False):\n",
    "        steps.append((\"scaler\", StandardScaler()))\n",
    "\n",
    "    clf_name = MODEL_CONFIG[\"classifier\"]\n",
    "    if clf_name == \"xgb\" and XGB_AVAILABLE:\n",
    "        clf = xgb.XGBClassifier(\n",
    "            **MODEL_CONFIG[\"params\"],\n",
    "            objective=\"binary:logistic\",\n",
    "            eval_metric=\"auc\",\n",
    "            use_label_encoder=False,\n",
    "        )\n",
    "    elif clf_name == \"hgb\":\n",
    "        hgb_params = {\n",
    "            \"max_depth\": None if MODEL_CONFIG[\"params\"].get(\"max_depth\", 0) <= 0 else MODEL_CONFIG[\"params\"][\"max_depth\"],\n",
    "            \"learning_rate\": MODEL_CONFIG[\"params\"].get(\"learning_rate\", 0.1),\n",
    "            \"max_iter\": MODEL_CONFIG[\"params\"].get(\"n_estimators\", 300),\n",
    "            \"l2_regularization\": MODEL_CONFIG[\"params\"].get(\"reg_lambda\", 0.0),\n",
    "            \"min_samples_leaf\": 20,\n",
    "            \"random_state\": GLOBAL_SEED,\n",
    "        }\n",
    "        clf = HistGradientBoostingClassifier(**hgb_params)\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported classifier in MODEL_CONFIG\")\n",
    "\n",
    "    steps.append((\"clf\", clf))\n",
    "    return Pipeline(steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b730511b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Training bin 5_15 [5, 15) GeV ===\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "Configured file not found: shapes_photon_10.txt",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 90\u001b[0m\n\u001b[1;32m     87\u001b[0m bin_label \u001b[38;5;241m=\u001b[39m BIN_LABELS[i]\n\u001b[1;32m     88\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m=== Training bin \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbin_label\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00met_min\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;132;01m{\u001b[39;00met_max\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m) GeV ===\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 90\u001b[0m df_bin_unweighted \u001b[38;5;241m=\u001b[39m \u001b[43mload_bin_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbin_label\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43met_min\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43met_max\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     92\u001b[0m \u001b[38;5;66;03m# Apply reweighting\u001b[39;00m\n\u001b[1;32m     93\u001b[0m df_bin \u001b[38;5;241m=\u001b[39m apply_eta_reweighting(df_bin_unweighted)\n",
      "Cell \u001b[0;32mIn[10], line 38\u001b[0m, in \u001b[0;36mload_bin_data\u001b[0;34m(bin_label, et_min, et_max)\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m file_type, path \u001b[38;5;129;01min\u001b[39;00m file_map\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m     37\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(path):\n\u001b[0;32m---> 38\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConfigured file not found: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     39\u001b[0m     df_i \u001b[38;5;241m=\u001b[39m load_single(path, names\u001b[38;5;241m=\u001b[39mCOLUMNS)\n\u001b[1;32m     40\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m file_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msignal\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: Configured file not found: shapes_photon_10.txt"
     ]
    }
   ],
   "source": [
    "# Section 6: Train One Model per Bin\n",
    "\n",
    "trained_pipelines = {}\n",
    "train_metrics = {}\n",
    "\n",
    "if TRAIN_SINGLE_MODEL:\n",
    "    print(\"=== Training a single model for all bins ===\")\n",
    "    \n",
    "    # 1. Load and combine data from all bins\n",
    "    all_dfs_unweighted = []\n",
    "    for i in range(len(BIN_EDGES) - 1):\n",
    "        et_min, et_max = BIN_EDGES[i], BIN_EDGES[i+1]\n",
    "        bin_label = BIN_LABELS[i]\n",
    "        print(f\"Loading data for bin {bin_label}...\")\n",
    "        df_bin_unweighted = load_bin_data(bin_label, et_min, et_max)\n",
    "        all_dfs_unweighted.append(df_bin_unweighted)\n",
    "    \n",
    "    df_all_unweighted = pd.concat(all_dfs_unweighted, ignore_index=True)\n",
    "    \n",
    "    # 2. Apply reweighting to the combined dataset\n",
    "    df_all = apply_eta_reweighting(df_all_unweighted)\n",
    "    \n",
    "    # 3. Get features and split into train/validation\n",
    "    feats = FEATURES if FEATURES else autodetect_features(df_all)\n",
    "    if not feats:\n",
    "        raise ValueError(\"No FEATURES found for combined dataset. Please define FEATURES explicitly.\")\n",
    "        \n",
    "    X_train, X_val, y_train, y_val, w_train, w_val = split_train_val_with_features(df_all, feats)\n",
    "\n",
    "    # 4. Build and train the single pipeline\n",
    "    pipe = build_pipeline()\n",
    "    t0 = time.time()\n",
    "    #if \"sample_weight\" in pipe.named_steps['clf'].fit.__code__.co_varnames:\n",
    "    pipe.fit(X_train, y_train, clf__sample_weight=w_train)\n",
    "    #else:\n",
    "    #    pipe.fit(X_train, y_train)\n",
    "    t1 = time.time()\n",
    "\n",
    "    # 5. Calculate training AUC\n",
    "    if hasattr(pipe.named_steps[\"clf\"], \"predict_proba\"):\n",
    "        y_train_proba = pipe.predict_proba(X_train)[:, 1]\n",
    "    else:\n",
    "        y_train_proba = pipe.decision_function(X_train)\n",
    "    auc_train = roc_auc_score(y_train, y_train_proba, sample_weight=w_train) if len(np.unique(y_train)) > 1 else np.nan\n",
    "\n",
    "    # 6. Store results for each bin for evaluation\n",
    "    df_val_all = df_all.loc[X_val.index]\n",
    "    \n",
    "    for i in range(len(BIN_EDGES) - 1):\n",
    "        et_min, et_max = BIN_EDGES[i], BIN_EDGES[i+1]\n",
    "        bin_label = BIN_LABELS[i]\n",
    "        \n",
    "        # Filter validation data for this bin\n",
    "        val_mask = (df_val_all[PT_COL] >= et_min) & (df_val_all[PT_COL] < et_max)\n",
    "        df_val_bin = df_val_all[val_mask]\n",
    "        \n",
    "        if df_val_bin.empty:\n",
    "            continue\n",
    "\n",
    "        X_val_bin = df_val_bin[feats]\n",
    "        y_val_bin = df_val_bin[LABEL_COL].astype(int).values\n",
    "        w_val_bin = df_val_bin[\"weight\"].to_numpy(dtype=np.float32)\n",
    "\n",
    "        trained_pipelines[bin_label] = {\n",
    "            \"pipeline\": pipe,  # The single trained pipeline\n",
    "            \"features\": feats,\n",
    "            \"X_val\": X_val_bin,\n",
    "            \"y_val\": y_val_bin,\n",
    "            \"w_val\": w_val_bin,\n",
    "            \"val_index\": X_val_bin.index,\n",
    "            \"df_bin\": df_all,  # Full dataframe for ISO correlation\n",
    "            \"df_bin_unweighted\": df_all_unweighted, # Full unweighted for QA\n",
    "        }\n",
    "        train_metrics[bin_label] = {\n",
    "            \"train_time_sec\": round(t1 - t0, 3),\n",
    "            \"n_train\": int(len(y_train)),\n",
    "            \"n_val\": int(len(y_val_bin)),\n",
    "            \"auc_train\": float(auc_train) if not np.isnan(auc_train) else np.nan,\n",
    "            \"et_min\": float(et_min),\n",
    "            \"et_max\": float(et_max),\n",
    "        }\n",
    "\n",
    "else:\n",
    "    # Original per-bin training loop\n",
    "    for i in range(len(BIN_EDGES) - 1):\n",
    "        et_min, et_max = BIN_EDGES[i], BIN_EDGES[i+1]\n",
    "        bin_label = BIN_LABELS[i]\n",
    "        print(f\"\\n=== Training bin {bin_label} [{et_min}, {et_max}) GeV ===\")\n",
    "\n",
    "        df_bin_unweighted = load_bin_data(bin_label, et_min, et_max)\n",
    "        \n",
    "        # Apply reweighting\n",
    "        df_bin = apply_eta_reweighting(df_bin_unweighted)\n",
    "        \n",
    "        feats = FEATURES if FEATURES else autodetect_features(df_bin)\n",
    "        if not feats:\n",
    "            raise ValueError(f\"No FEATURES found for bin {bin_label}. Please define FEATURES explicitly.\")\n",
    "\n",
    "        X_train, X_val, y_train, y_val, w_train, w_val = split_train_val_with_features(df_bin, feats)\n",
    "\n",
    "        print(w_train)\n",
    "\n",
    "        pipe = build_pipeline()\n",
    "        t0 = time.time()\n",
    "        \n",
    "        # Pass sample weights to the fit method\n",
    "        #if \"sample_weight\" in pipe.named_steps['clf'].fit.__code__.co_varnames:\n",
    "        print(\"Train with sample weights\")\n",
    "        pipe.fit(X_train, y_train, clf__sample_weight=w_train)\n",
    "            \n",
    "        #else:\n",
    "        #    pipe.fit(X_train, y_train)\n",
    "            \n",
    "        t1 = time.time()\n",
    "\n",
    "        # Basic training AUC on train split\n",
    "        if hasattr(pipe.named_steps[\"clf\"], \"predict_proba\"):\n",
    "            y_train_proba = pipe.predict_proba(X_train)[:, 1]\n",
    "        else:\n",
    "            y_train_proba = pipe.decision_function(X_train)\n",
    "        auc_train = roc_auc_score(y_train, y_train_proba, sample_weight=w_train) if len(np.unique(y_train)) > 1 else np.nan\n",
    "\n",
    "        trained_pipelines[bin_label] = {\n",
    "            \"pipeline\": pipe,\n",
    "            \"features\": feats,\n",
    "            \"X_val\": X_val,\n",
    "            \"y_val\": y_val,\n",
    "            \"w_val\": w_val,\n",
    "            \"val_index\": X_val.index,  # store for alignment\n",
    "            \"df_bin\": df_bin,  # keep for ISO correlation reference\n",
    "            \"df_bin_unweighted\": df_bin_unweighted, # For QA plots\n",
    "        }\n",
    "        train_metrics[bin_label] = {\n",
    "            \"train_time_sec\": round(t1 - t0, 3),\n",
    "            \"n_train\": int(len(y_train)),\n",
    "            \"n_val\": int(len(y_val)),\n",
    "            \"auc_train\": float(auc_train) if not np.isnan(auc_train) else np.nan,\n",
    "            \"et_min\": float(et_min),\n",
    "            \"et_max\": float(et_max),\n",
    "        }\n",
    "\n",
    "print(\"\\nTraining complete for all bins.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bfa41ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Section 7: Persist Trained Pipelines and Metadata\n",
    "\n",
    "if TRAIN_SINGLE_MODEL and trained_pipelines:\n",
    "    # Save the single model once\n",
    "    first_bin = next(iter(trained_pipelines.keys()))\n",
    "    pipe = trained_pipelines[first_bin][\"pipeline\"]\n",
    "    feats = trained_pipelines[first_bin][\"features\"]\n",
    "    \n",
    "    model_path = OUT_DIR / \"model_single.joblib\"\n",
    "    joblib.dump(pipe, model_path)\n",
    "\n",
    "    # Save metadata for each bin, referencing the single model\n",
    "    for bin_label in BIN_LABELS:\n",
    "        if bin_label not in trained_pipelines:\n",
    "            continue\n",
    "        meta = {\n",
    "            \"bin_label\": bin_label,\n",
    "            \"trained_as_single_model\": True,\n",
    "            \"single_model_path\": str(model_path),\n",
    "            \"bin_edges\": BIN_EDGES,\n",
    "            \"pt_col\": PT_COL,\n",
    "            \"iso_col\": ISO_COL,\n",
    "            \"label_col\": LABEL_COL,\n",
    "            \"features\": feats,\n",
    "            \"train_size\": TRAIN_SIZE,\n",
    "            \"random_seed\": GLOBAL_SEED,\n",
    "            \"model_config\": MODEL_CONFIG,\n",
    "            \"train_metrics\": train_metrics.get(bin_label, {}),\n",
    "            \"input_files\": PER_BIN_FILES.get(bin_label, []),\n",
    "        }\n",
    "        with open(OUT_DIR / f\"metadata_{bin_label}.json\", \"w\") as f:\n",
    "            json.dump(meta, f, indent=2)\n",
    "else:\n",
    "    # Original logic for per-bin models\n",
    "    for bin_label in BIN_LABELS:\n",
    "        if bin_label not in trained_pipelines:\n",
    "            continue\n",
    "        entry = trained_pipelines[bin_label]\n",
    "        pipe = entry[\"pipeline\"]\n",
    "        feats = entry[\"features\"]\n",
    "\n",
    "        model_path = OUT_DIR / f\"model_{bin_label}.joblib\"\n",
    "        joblib.dump(pipe, model_path)\n",
    "\n",
    "        meta = {\n",
    "            \"bin_label\": bin_label,\n",
    "            \"trained_as_single_model\": False,\n",
    "            \"bin_edges\": BIN_EDGES,\n",
    "            \"pt_col\": PT_COL,\n",
    "            \"iso_col\": ISO_COL,\n",
    "            \"label_col\": LABEL_COL,\n",
    "            \"features\": feats,\n",
    "            \"train_size\": TRAIN_SIZE,\n",
    "            \"random_seed\": GLOBAL_SEED,\n",
    "            \"model_config\": MODEL_CONFIG,\n",
    "            \"train_metrics\": train_metrics.get(bin_label, {}),\n",
    "            \"input_files\": PER_BIN_FILES.get(bin_label, []),\n",
    "        }\n",
    "        with open(OUT_DIR / f\"metadata_{bin_label}.json\", \"w\") as f:\n",
    "            json.dump(meta, f, indent=2)\n",
    "\n",
    "print(f\"Saved models and metadata to: {OUT_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ee06df2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Section 6.5: Reweighting QA Plots\n",
    "\n",
    "if TRAIN_SINGLE_MODEL and trained_pipelines:\n",
    "    # For a single model, show QA for the combined dataset\n",
    "    first_bin = next(iter(trained_pipelines.keys()))\n",
    "    df_unweighted = trained_pipelines[first_bin][\"df_bin_unweighted\"]\n",
    "    df_weighted = trained_pipelines[first_bin][\"df_bin\"]\n",
    "    plot_reweighting_qa(df_unweighted, df_weighted, \"all_bins_combined\")\n",
    "else:\n",
    "    # Original per-bin QA plots\n",
    "    for bin_label in BIN_LABELS:\n",
    "        if bin_label in trained_pipelines:\n",
    "            df_unweighted = trained_pipelines[bin_label][\"df_bin_unweighted\"]\n",
    "            df_weighted = trained_pipelines[bin_label][\"df_bin\"]\n",
    "            plot_reweighting_qa(df_unweighted, df_weighted, bin_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65fc0fea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Section 8: Evaluate AUC per pT/cluster_Et Bin\n",
    "\n",
    "val_results = {}\n",
    "\n",
    "for bin_label in BIN_LABELS:\n",
    "    if bin_label not in trained_pipelines:\n",
    "        continue\n",
    "    entry = trained_pipelines[bin_label]\n",
    "    pipe = entry[\"pipeline\"]\n",
    "    X_val, y_val, w_val = entry[\"X_val\"], entry[\"y_val\"], entry[\"w_val\"]\n",
    "\n",
    "    if hasattr(pipe.named_steps[\"clf\"], \"predict_proba\"):\n",
    "        y_proba = pipe.predict_proba(X_val)[:, 1]\n",
    "    else:\n",
    "        y_proba = pipe.decision_function(X_val)\n",
    "\n",
    "    auc = roc_auc_score(y_val, y_proba, sample_weight=w_val) if len(np.unique(y_val)) > 1 else np.nan\n",
    "    fpr, tpr, thr = roc_curve(y_val, y_proba, sample_weight=w_val) if len(np.unique(y_val)) > 1 else (None, None, None)\n",
    "\n",
    "    val_results[bin_label] = {\n",
    "        \"auc_val\": float(auc) if not np.isnan(auc) else np.nan,\n",
    "        \"fpr\": fpr,\n",
    "        \"tpr\": tpr,\n",
    "        \"thresholds\": thr,\n",
    "        \"n_val\": int(len(y_val)),\n",
    "    }\n",
    "\n",
    "val_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbb51c51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Section 9: Compute isoET vs BDT Score Correlation per Bin\n",
    "\n",
    "correlation_results = {}\n",
    "\n",
    "for bin_label in BIN_LABELS:\n",
    "    if bin_label not in trained_pipelines:\n",
    "        continue\n",
    "    entry = trained_pipelines[bin_label]\n",
    "    pipe = entry[\"pipeline\"]\n",
    "    X_val, y_val, df_bin, val_index, w_val = entry[\"X_val\"], entry[\"y_val\"], entry[\"df_bin\"], entry[\"val_index\"], entry[\"w_val\"]\n",
    "\n",
    "    # isoET values for the validation rows\n",
    "    iso_val = df_bin.loc[val_index, ISO_COL].to_numpy()\n",
    "\n",
    "    # Predicted prob on validation set\n",
    "    if hasattr(pipe.named_steps[\"clf\"], \"predict_proba\"):\n",
    "        y_proba = pipe.predict_proba(X_val)[:, 1]\n",
    "    else:\n",
    "        y_proba = pipe.decision_function(X_val)\n",
    "\n",
    "    # Drop any NaNs\n",
    "    mask = np.isfinite(iso_val) & np.isfinite(y_proba) & np.isfinite(w_val)\n",
    "    iso_c = iso_val[mask]\n",
    "    proba_c = y_proba[mask]\n",
    "    w_c = w_val[mask]\n",
    "\n",
    "    # Weighted Pearson correlation\n",
    "    def weighted_pearson(x, y, w):\n",
    "        mx = np.average(x, weights=w)\n",
    "        my = np.average(y, weights=w)\n",
    "        cov = np.average((x - mx) * (y - my), weights=w)\n",
    "        sx = np.sqrt(np.average((x - mx)**2, weights=w))\n",
    "        sy = np.sqrt(np.average((y - my)**2, weights=w))\n",
    "        if sx * sy == 0:\n",
    "            return np.nan\n",
    "        return cov / (sx * sy)\n",
    "\n",
    "    pearson_r = weighted_pearson(iso_c, proba_c, w_c) if len(iso_c) > 1 else np.nan\n",
    "    # Spearman correlation is rank-based and doesn't have a standard weighted version\n",
    "    spearman_rho, spearman_p = stats.spearmanr(iso_c, proba_c) if len(iso_c) > 1 else (np.nan, np.nan)\n",
    "\n",
    "    correlation_results[bin_label] = {\n",
    "        \"pearson_r\": float(pearson_r) if np.isfinite(pearson_r) else np.nan,\n",
    "        \"pearson_p\": np.nan, # p-value for weighted correlation is complex\n",
    "        \"spearman_rho\": float(spearman_rho) if np.isfinite(spearman_rho) else np.nan,\n",
    "        \"spearman_p\": float(spearman_p) if np.isfinite(spearman_p) else np.nan,\n",
    "        \"n_pairs\": int(np.sum(mask)),\n",
    "    }\n",
    "\n",
    "correlation_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "820334a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Section 10: Visualize AUC and isoET–BDT Relationships\n",
    "\n",
    "# --- Helper function for profile plots ---\n",
    "def plot_feature_profile(df_bin, val_index, y_proba, feature_name, bin_label, w_val):\n",
    "    \"\"\"\n",
    "    Generates a 2D histogram and profile plot for a given feature vs. BDT score.\n",
    "    - Calculates and annotates the weighted Pearson correlation between the feature and BDT score.\n",
    "    - Plots the feature on the y-axis except when the feature is isoET (ISO_COL); in that case, BDT score is on the y-axis.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Separate signal and background in the validation set\n",
    "    val_df = df_bin.loc[val_index].copy()\n",
    "    val_df['bdt_score'] = y_proba\n",
    "    val_df['weight'] = w_val\n",
    "\n",
    "    sig_df = val_df[val_df[LABEL_COL] == 1]\n",
    "    bkg_df = val_df[val_df[LABEL_COL] == 0]\n",
    "\n",
    "    # Axis rule: feature on y-axis, except for isoET -> swap axes\n",
    "    invert_axes = not (feature_name == ISO_COL)\n",
    "\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6), sharey=invert_axes)\n",
    "    title_xy = f\"BDT Score vs. {feature_name}\" if invert_axes else f\"{feature_name} vs. BDT Score\"\n",
    "    fig.suptitle(f\"Profile of {title_xy} (Bin: {bin_label})\", fontsize=16)\n",
    "\n",
    "    score_bins = np.linspace(0, 1, 41)\n",
    "\n",
    "    # Weighted Pearson correlation helper\n",
    "    def weighted_pearson(x, y, w):\n",
    "        mask = np.isfinite(x) & np.isfinite(y) & np.isfinite(w)\n",
    "        x, y, w = x[mask], y[mask], w[mask]\n",
    "        if x.size < 2:\n",
    "            return np.nan\n",
    "        w_sum = np.sum(w)\n",
    "        mx = np.sum(w * x) / w_sum\n",
    "        my = np.sum(w * y) / w_sum\n",
    "        cov = np.sum(w * (x - mx) * (y - my)) / w_sum\n",
    "        sx = np.sqrt(np.sum(w * (x - mx) ** 2) / w_sum)\n",
    "        sy = np.sqrt(np.sum(w * (y - my) ** 2) / w_sum)\n",
    "        if sx == 0 or sy == 0:\n",
    "            return np.nan\n",
    "        return cov / (sx * sy)\n",
    "\n",
    "    # --- Signal Plot ---\n",
    "    if invert_axes:\n",
    "        x_sig = sig_df[feature_name]\n",
    "        y_sig = sig_df['bdt_score']\n",
    "        w_sig = sig_df['weight']\n",
    "        # Determine feature binning for x-axis\n",
    "        if len(x_sig) > 0:\n",
    "            x_min, x_max = np.nanpercentile(x_sig, [1, 99])\n",
    "            if not np.isfinite(x_min) or not np.isfinite(x_max) or x_min == x_max:\n",
    "                x_min, x_max = np.nanmin(x_sig), np.nanmax(x_sig)\n",
    "        else:\n",
    "            x_min, x_max = 0.0, 1.0\n",
    "        feat_bins = np.linspace(x_min, x_max, 51)\n",
    "        ax1.hist2d(x_sig, y_sig, bins=[feat_bins, score_bins], cmin=1, weights=w_sig, cmap='viridis')\n",
    "        # Profile (mean of y in x-bins)\n",
    "        bin_centers = 0.5 * (feat_bins[:-1] + feat_bins[1:])\n",
    "        bin_means, _, _ = stats.binned_statistic(x_sig, y_sig, statistic='mean', bins=feat_bins)\n",
    "        ax1.plot(bin_centers, bin_means, 'r-o', lw=2, label='Profile')\n",
    "        ax1.set_xlabel(feature_name)\n",
    "        ax1.set_ylabel(\"BDT Score\")\n",
    "    else:\n",
    "        x_sig = sig_df['bdt_score']\n",
    "        y_sig = sig_df[feature_name]\n",
    "        w_sig = sig_df['weight']\n",
    "        ax1.hist2d(x_sig, y_sig, bins=[score_bins, 50], cmin=1, weights=w_sig, cmap='viridis')\n",
    "        # Profile (mean of y in BDT-score bins)\n",
    "        bin_centers = (score_bins[:-1] + score_bins[1:]) / 2\n",
    "        bin_means, _, _ = stats.binned_statistic(x_sig, y_sig, statistic='mean', bins=score_bins)\n",
    "        ax1.plot(bin_centers, bin_means, 'r-o', lw=2, label='Profile')\n",
    "        ax1.set_xlabel(\"BDT Score\")\n",
    "        ax1.set_ylabel(feature_name)\n",
    "\n",
    "    r_sig = weighted_pearson(x_sig, y_sig, w_sig)\n",
    "    ax1.set_title(\"Signal\")\n",
    "    ax1.legend()\n",
    "    ax1.grid(True)\n",
    "    ax1.text(0.02, 0.98, f\"r = {r_sig:.3f}\", transform=ax1.transAxes, ha='left', va='top',\n",
    "             bbox=dict(boxstyle='round', facecolor='white', alpha=0.7))\n",
    "\n",
    "    # --- Background Plot ---\n",
    "    if invert_axes:\n",
    "        x_bkg = bkg_df[feature_name]\n",
    "        y_bkg = bkg_df['bdt_score']\n",
    "        w_bkg = bkg_df['weight']\n",
    "        # Reuse feat_bins where possible; recompute if empty\n",
    "        if len(x_bkg) > 0 and np.isfinite(np.nanmin(x_bkg)) and np.isfinite(np.nanmax(x_bkg)):\n",
    "            ax2.hist2d(x_bkg, y_bkg, bins=[feat_bins, score_bins], cmin=1, weights=w_bkg, cmap='viridis')\n",
    "            bin_centers_bkg = 0.5 * (feat_bins[:-1] + feat_bins[1:])\n",
    "            bin_means_bkg, _, _ = stats.binned_statistic(x_bkg, y_bkg, statistic='mean', bins=feat_bins)\n",
    "        else:\n",
    "            ax2.hist2d(x_bkg, y_bkg, bins=[50, score_bins], cmin=1, weights=w_bkg, cmap='viridis')\n",
    "            # approximate centers from data range\n",
    "            xb_min, xb_max = (np.nanmin(x_bkg) if len(x_bkg) else 0.0), (np.nanmax(x_bkg) if len(x_bkg) else 1.0)\n",
    "            feat_bins_bkg = np.linspace(xb_min, xb_max, 51)\n",
    "            bin_centers_bkg = 0.5 * (feat_bins_bkg[:-1] + feat_bins_bkg[1:])\n",
    "            bin_means_bkg, _, _ = stats.binned_statistic(x_bkg, y_bkg, statistic='mean', bins=feat_bins_bkg)\n",
    "        ax2.plot(bin_centers_bkg, bin_means_bkg, 'r-o', lw=2, label='Profile')\n",
    "        ax2.set_xlabel(feature_name)\n",
    "        ax2.set_ylabel(\"BDT Score\")\n",
    "    else:\n",
    "        x_bkg = bkg_df['bdt_score']\n",
    "        y_bkg = bkg_df[feature_name]\n",
    "        w_bkg = bkg_df['weight']\n",
    "        ax2.hist2d(x_bkg, y_bkg, bins=[score_bins, 50], cmin=1, weights=w_bkg, cmap='viridis')\n",
    "        bin_centers_bkg = (score_bins[:-1] + score_bins[1:]) / 2\n",
    "        bin_means_bkg, _, _ = stats.binned_statistic(x_bkg, y_bkg, statistic='mean', bins=score_bins)\n",
    "        ax2.plot(bin_centers_bkg, bin_means_bkg, 'r-o', lw=2, label='Profile')\n",
    "        ax2.set_xlabel(\"BDT Score\")\n",
    "        ax2.set_ylabel(feature_name)\n",
    "\n",
    "    r_bkg = weighted_pearson(x_bkg, y_bkg, w_bkg)\n",
    "    ax2.set_title(\"Background\")\n",
    "    ax2.legend()\n",
    "    ax2.grid(True)\n",
    "    ax2.text(0.02, 0.98, f\"r = {r_bkg:.3f}\", transform=ax2.transAxes, ha='left', va='top',\n",
    "             bbox=dict(boxstyle='round', facecolor='white', alpha=0.7))\n",
    "    \n",
    "    plt.tight_layout(rect=[0, 0, 1, 0.96])\n",
    "    plt.show()\n",
    "\n",
    "# --- Overlayed profile plots across bins ---\n",
    "def overlay_feature_profiles(trained_pipelines, feature_name, ISO_COL, LABEL_COL, BIN_LABELS):\n",
    "    \"\"\"\n",
    "    Draw profile curves overlaid for all bins:\n",
    "      - Left: signal profiles\n",
    "      - Right: background profiles\n",
    "    Axis rule: feature on x-axis, BDT score on y-axis,\n",
    "               EXCEPT when feature is isoET (ISO_COL) -> swap axes.\n",
    "    \"\"\"\n",
    "    # Collect per-bin arrays to define a common x-binning\n",
    "    xs_sig_all, xs_bkg_all = [], []\n",
    "    profiles_sig, profiles_bkg = {} , {}\n",
    "\n",
    "    invert_axes = not (feature_name == ISO_COL)  # same rule as your helper\n",
    "    score_bins = np.linspace(0, 1, 41)           # used when x is BDT score\n",
    "\n",
    "    # Pass 1: gather all x values per class for global bin edges\n",
    "    for bin_label in BIN_LABELS:\n",
    "        if bin_label not in trained_pipelines:\n",
    "            continue\n",
    "        entry = trained_pipelines[bin_label]\n",
    "        pipe = entry[\"pipeline\"]\n",
    "        X_val = entry[\"X_val\"]\n",
    "        df_bin = entry[\"df_bin\"].loc[entry[\"val_index\"]].copy()\n",
    "        w_val = entry[\"w_val\"]\n",
    "\n",
    "        # get scores\n",
    "        if hasattr(pipe.named_steps[\"clf\"], \"predict_proba\"):\n",
    "            y_proba = pipe.predict_proba(X_val)[:, 1]\n",
    "        else:\n",
    "            y_proba = pipe.decision_function(X_val)\n",
    "\n",
    "        df_bin[\"bdt_score\"] = y_proba\n",
    "        df_bin[\"weight\"] = w_val\n",
    "\n",
    "        sig = df_bin[df_bin[LABEL_COL] == 1]\n",
    "        bkg = df_bin[df_bin[LABEL_COL] == 0]\n",
    "\n",
    "        if invert_axes:\n",
    "            # x = feature, y = score\n",
    "            xs_sig_all.append(sig[feature_name].to_numpy())\n",
    "            xs_bkg_all.append(bkg[feature_name].to_numpy())\n",
    "        else:\n",
    "            # x = score, y = feature\n",
    "            xs_sig_all.append(sig[\"bdt_score\"].to_numpy())\n",
    "            xs_bkg_all.append(bkg[\"bdt_score\"].to_numpy())\n",
    "\n",
    "    # Define common x-bins per class (robust to outliers)\n",
    "    def robust_bins(arr_list, nbins=50):\n",
    "        if len(arr_list) == 0:\n",
    "            return np.linspace(0, 1, nbins+1)\n",
    "        x_all = np.concatenate([a[np.isfinite(a)] for a in arr_list if a is not None])\n",
    "        if x_all.size == 0:\n",
    "            return np.linspace(0, 1, nbins+1)\n",
    "        x1, x99 = np.nanpercentile(x_all, [1, 99])\n",
    "        if not np.isfinite(x1) or not np.isfinite(x99) or x1 == x99:\n",
    "            x1, x99 = np.nanmin(x_all), np.nanmax(x_all)\n",
    "            if x1 == x99:\n",
    "                x1, x99 = x1 - 0.5, x1 + 0.5\n",
    "        return np.linspace(x1, x99, nbins+1)\n",
    "\n",
    "    if invert_axes:\n",
    "        # feature on x for both panels\n",
    "        xbins_sig = robust_bins(xs_sig_all, nbins=50)\n",
    "        xbins_bkg = robust_bins(xs_bkg_all, nbins=50)\n",
    "    else:\n",
    "        # BDT score on x for both panels\n",
    "        xbins_sig = score_bins\n",
    "        xbins_bkg = score_bins\n",
    "\n",
    "    # Helper: unweighted mean per x-bin (matches your current behavior)\n",
    "    def binned_mean(x, y, bins):\n",
    "        # returns bin centers, and mean of y in each x-bin\n",
    "        inds = np.digitize(x, bins) - 1\n",
    "        centers = 0.5 * (bins[:-1] + bins[1:])\n",
    "        means = np.full(len(centers), np.nan)\n",
    "        for bi in range(len(centers)):\n",
    "            m = inds == bi\n",
    "            if np.any(m):\n",
    "                means[bi] = np.nanmean(y[m])\n",
    "        return centers, means\n",
    "\n",
    "    # Pass 2: compute profile per bin (signal & background)\n",
    "    for bin_label in BIN_LABELS:\n",
    "        if bin_label not in trained_pipelines:\n",
    "            continue\n",
    "        entry = trained_pipelines[bin_label]\n",
    "        pipe = entry[\"pipeline\"]\n",
    "        X_val = entry[\"X_val\"]\n",
    "        df_bin = entry[\"df_bin\"].loc[entry[\"val_index\"]].copy()\n",
    "\n",
    "        if hasattr(pipe.named_steps[\"clf\"], \"predict_proba\"):\n",
    "            y_proba = pipe.predict_proba(X_val)[:, 1]\n",
    "        else:\n",
    "            y_proba = pipe.decision_function(X_val)\n",
    "\n",
    "        df_bin[\"bdt_score\"] = y_proba\n",
    "\n",
    "        sig = df_bin[df_bin[LABEL_COL] == 1]\n",
    "        bkg = df_bin[df_bin[LABEL_COL] == 0]\n",
    "\n",
    "        if invert_axes:\n",
    "            x_s, y_s = sig[feature_name].to_numpy(), sig[\"bdt_score\"].to_numpy()\n",
    "            x_b, y_b = bkg[feature_name].to_numpy(), bkg[\"bdt_score\"].to_numpy()\n",
    "        else:\n",
    "            x_s, y_s = sig[\"bdt_score\"].to_numpy(), sig[feature_name].to_numpy()\n",
    "            x_b, y_b = bkg[\"bdt_score\"].to_numpy(), bkg[feature_name].to_numpy()\n",
    "\n",
    "        cs, ms = binned_mean(x_s, y_s, xbins_sig)\n",
    "        cb, mb = binned_mean(x_b, y_b, xbins_bkg)\n",
    "\n",
    "        profiles_sig[bin_label] = (cs, ms)\n",
    "        profiles_bkg[bin_label] = (cb, mb)\n",
    "\n",
    "    # Plot overlays\n",
    "    fig, (axS, axB) = plt.subplots(1, 2, figsize=(16, 6), sharey=not invert_axes)\n",
    "    if invert_axes:\n",
    "        axS.set_xlabel(feature_name); axS.set_ylabel(\"BDT Score\")\n",
    "        axB.set_xlabel(feature_name); axB.set_ylabel(\"BDT Score\")\n",
    "        supt = f\"Overlayed Profiles: BDT vs {feature_name}\"\n",
    "    else:\n",
    "        axS.set_xlabel(\"BDT Score\");   axS.set_ylabel(feature_name)\n",
    "        axB.set_xlabel(\"BDT Score\");   axB.set_ylabel(feature_name)\n",
    "        supt = f\"Overlayed Profiles: {feature_name} vs BDT\"\n",
    "\n",
    "    for bin_label, (c, m) in profiles_sig.items():\n",
    "        axS.plot(c, m, marker='o', lw=2, label=str(bin_label))\n",
    "    axS.set_title(\"Signal\"); axS.grid(True); axS.legend()\n",
    "\n",
    "    for bin_label, (c, m) in profiles_bkg.items():\n",
    "        axB.plot(c, m, marker='o', lw=2, label=str(bin_label))\n",
    "    axB.set_title(\"Background\"); axB.grid(True); axB.legend()\n",
    "\n",
    "    fig.suptitle(supt, fontsize=16)\n",
    "    plt.tight_layout(rect=[0, 0, 1, 0.95])\n",
    "    plt.show()\n",
    "\n",
    "def plot_bdt_score_overlay_per_bin(trained_pipelines, BIN_LABELS, LABEL_COL, n_bins=50, density=True):\n",
    "    \"\"\"\n",
    "    For each bin in BIN_LABELS, overlay the weighted BDT-score distributions of signal and background.\n",
    "    - Uses per-bin common bin edges (computed from S∪B scores) so curves are comparable.\n",
    "    - Normalizes to density by default (area=1 for each class in that bin), controlled by `density`.\n",
    "    \"\"\"\n",
    "    for bin_label in BIN_LABELS:\n",
    "        if bin_label not in trained_pipelines:\n",
    "            continue\n",
    "        entry = trained_pipelines[bin_label]\n",
    "        pipe = entry[\"pipeline\"]\n",
    "        X_val = entry[\"X_val\"]\n",
    "        df_val = entry[\"df_bin\"].loc[entry[\"val_index\"]].copy()\n",
    "        w_val = entry[\"w_val\"]\n",
    "\n",
    "        # Get BDT scores\n",
    "        if hasattr(pipe.named_steps[\"clf\"], \"predict_proba\"):\n",
    "            scores = pipe.predict_proba(X_val)[:, 1]\n",
    "            score_label = \"BDT Score\"\n",
    "            # Prefer [0,1] bins for probabilities\n",
    "            bin_edges = np.linspace(0, 1, n_bins + 1)\n",
    "        else:\n",
    "            scores = pipe.decision_function(X_val)\n",
    "            score_label = \"BDT Score (decision_function)\"\n",
    "            # Robust binning from combined S∪B (1–99% to avoid outliers)\n",
    "            p1, p99 = np.nanpercentile(scores[np.isfinite(scores)], [1, 99])\n",
    "            if not np.isfinite(p1) or not np.isfinite(p99) or p1 == p99:\n",
    "                p1, p99 = np.nanmin(scores), np.nanmax(scores)\n",
    "                if p1 == p99:\n",
    "                    p1, p99 = p1 - 0.5, p1 + 0.5\n",
    "            bin_edges = np.linspace(p1, p99, n_bins + 1)\n",
    "\n",
    "        df_val[\"score\"] = scores\n",
    "        df_val[\"w\"] = w_val\n",
    "\n",
    "        sig = df_val[df_val[LABEL_COL] == 1]\n",
    "        bkg = df_val[df_val[LABEL_COL] == 0]\n",
    "\n",
    "        # Build the plot\n",
    "        plt.figure(figsize=(7, 5))\n",
    "        # Background first so signal can overlay clearly\n",
    "        plt.hist(\n",
    "            bkg[\"score\"].values,\n",
    "            bins=bin_edges,\n",
    "            weights=bkg[\"w\"].values,\n",
    "            histtype=\"stepfilled\",\n",
    "            alpha=0.35,\n",
    "            label=\"Background\",\n",
    "            density=density,\n",
    "        )\n",
    "        plt.hist(\n",
    "            sig[\"score\"].values,\n",
    "            bins=bin_edges,\n",
    "            weights=sig[\"w\"].values,\n",
    "            histtype=\"step\",\n",
    "            linewidth=2,\n",
    "            label=\"Signal\",\n",
    "            density=density,\n",
    "        )\n",
    "\n",
    "        # Styling\n",
    "        plt.title(f\"BDT Score Distribution — Bin: {bin_label}\")\n",
    "        plt.xlabel(score_label)\n",
    "        plt.ylabel(\"Density\" if density else \"Weighted counts\")\n",
    "        plt.grid(True, alpha=0.35)\n",
    "        plt.legend()\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79efcc78",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- AUC Bar Plot ---\n",
    "auc_data = [(b, val_results[b][\"auc_val\"]) for b in BIN_LABELS if b in val_results]\n",
    "if auc_data:\n",
    "    bins_, aucs_ = zip(*auc_data)\n",
    "    plt.figure()\n",
    "    sns.barplot(x=list(bins_), y=list(aucs_), color=\"steelblue\")\n",
    "    plt.ylabel(\"Validation AUC\")\n",
    "    plt.xlabel(\"cluster_Et bin [GeV]\")\n",
    "    plt.title(\"AUC by pT bin\")\n",
    "    plt.ylim(0.5, 1.0)\n",
    "    for i, v in enumerate(aucs_):\n",
    "        plt.text(i, v + 0.01, f\"{v:.3f}\", ha=\"center\")\n",
    "    plt.show()\n",
    "\n",
    "# --- Combined ROC Curves ---\n",
    "plt.figure(figsize=(8, 8))\n",
    "for bin_label in BIN_LABELS:\n",
    "    if bin_label not in val_results:\n",
    "        continue\n",
    "    r = val_results[bin_label]\n",
    "    if r[\"fpr\"] is not None and r[\"tpr\"] is not None:\n",
    "        plt.plot(r[\"fpr\"], r[\"tpr\"], label=f\"{bin_label} (AUC={r['auc_val']:.3f})\")\n",
    "\n",
    "plt.plot([0, 1], [0, 1], \"k--\", alpha=0.5, label=\"Chance\")\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.ylabel(\"True Positive Rate\")\n",
    "plt.title(\"ROC Curves for all pT Bins\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.axis('square')\n",
    "plt.xlim(0, 1)\n",
    "plt.ylim(0, 1)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "032c7065",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# --- Profile Plots for isoET and all Training Features ---\n",
    "for bin_label in BIN_LABELS:\n",
    "    if bin_label not in trained_pipelines:\n",
    "        continue\n",
    "    entry = trained_pipelines[bin_label]\n",
    "    pipe = entry[\"pipeline\"]\n",
    "    X_val = entry[\"X_val\"]\n",
    "    df_bin = entry[\"df_bin\"]\n",
    "    val_index = entry[\"val_index\"]\n",
    "    w_val = entry[\"w_val\"]\n",
    "    \n",
    "    if hasattr(pipe.named_steps[\"clf\"], \"predict_proba\"):\n",
    "        y_proba = pipe.predict_proba(X_val)[:, 1]\n",
    "    else:\n",
    "        y_proba = pipe.decision_function(X_val)\n",
    "\n",
    "    # Plot for recoisoET\n",
    "    plot_feature_profile(df_bin, val_index, y_proba, ISO_COL, bin_label, w_val)\n",
    "    \n",
    "    # Plot for cluster_Et\n",
    "    plot_feature_profile(df_bin, val_index, y_proba, PT_COL, bin_label, w_val)\n",
    "\n",
    "    # Plots for all training features\n",
    "    for feature in entry[\"features\"]:\n",
    "        plot_feature_profile(df_bin, val_index, y_proba, feature, bin_label, w_val)\n",
    "for feature in trained_pipelines[next(iter(trained_pipelines))][\"features\"]:\n",
    "    overlay_feature_profiles(trained_pipelines, feature, ISO_COL, LABEL_COL, BIN_LABELS)\n",
    "overlay_feature_profiles(trained_pipelines, PT_COL, ISO_COL, LABEL_COL, BIN_LABELS)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "390e96dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plot_bdt_score_overlay_per_bin(\n",
    "    trained_pipelines=trained_pipelines,\n",
    "    BIN_LABELS=BIN_LABELS,\n",
    "    LABEL_COL=LABEL_COL,\n",
    "    n_bins=50,       # tweak if you want smoother/rougher histograms\n",
    "    density=True     # set False to see weighted counts instead\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64ae9db7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Section 11: Summarize and Save Metrics Table\n",
    "\n",
    "rows = []\n",
    "for bin_label in BIN_LABELS:\n",
    "    row = {\"bin\": bin_label}\n",
    "    # Training metrics\n",
    "    row.update(train_metrics.get(bin_label, {}))\n",
    "    # Validation AUC\n",
    "    if bin_label in val_results:\n",
    "        row[\"auc_val\"] = val_results[bin_label][\"auc_val\"]\n",
    "        row[\"n_val\"] = val_results[bin_label][\"n_val\"]\n",
    "    else:\n",
    "        row[\"auc_val\"] = np.nan\n",
    "        row[\"n_val\"] = 0\n",
    "    # Correlations\n",
    "    if bin_label in correlation_results:\n",
    "        row.update(correlation_results[bin_label])\n",
    "    else:\n",
    "        row.update({\n",
    "            \"pearson_r\": np.nan,\n",
    "            \"pearson_p\": np.nan,\n",
    "            \"spearman_rho\": np.nan,\n",
    "            \"spearman_p\": np.nan,\n",
    "            \"n_pairs\": 0,\n",
    "        })\n",
    "    rows.append(row)\n",
    "\n",
    "metrics_df = pd.DataFrame(rows)\n",
    "metrics_df = metrics_df[[\n",
    "    \"bin\", \"et_min\", \"et_max\", \"n_train\", \"n_val\", \"auc_train\", \"auc_val\",\n",
    "    \"pearson_r\", \"pearson_p\", \"spearman_rho\", \"spearman_p\", \"n_pairs\", \"train_time_sec\"\n",
    "]].sort_values(\"bin\")\n",
    "\n",
    "metrics_path = OUT_DIR / \"per_bin_metrics.csv\"\n",
    "metrics_df.to_csv(metrics_path, index=False)\n",
    "\n",
    "print(\"Metrics saved:\", metrics_path)\n",
    "metrics_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "663346c6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
