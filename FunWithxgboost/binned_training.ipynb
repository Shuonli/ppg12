{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1263952d",
   "metadata": {},
   "source": [
    "# Binned Training Notebook (Training-Only)\n",
    "\n",
    "Train one model per cluster_Et bin using separate input files per bin. After training, compute per-bin AUC and the correlation between isoET and the model BDT score (predicted probability).\n",
    "\n",
    "Bins: [5, 15, 25, 40] (GeV) — models trained on [5–15), [15–25), [25–40).\n",
    "\n",
    "Outputs per bin:\n",
    "- Trained model artifact (joblib)\n",
    "- Metadata JSON\n",
    "- Validation AUC\n",
    "- isoET vs BDT score correlation (Pearson/Spearman)\n",
    "- Plots: AUC by bin, isoET–BDT hexbin/scatter, optional ROC curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbb4e346",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Section 1: Import Libraries and Set Seed\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import roc_auc_score, roc_curve\n",
    "from sklearn.ensemble import HistGradientBoostingClassifier\n",
    "\n",
    "# Optional: XGBoost (preferred if available)\n",
    "try:\n",
    "    import xgboost as xgb\n",
    "    XGB_AVAILABLE = True\n",
    "except Exception:\n",
    "    xgb = None\n",
    "    XGB_AVAILABLE = False\n",
    "\n",
    "from scipy import stats\n",
    "import joblib\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Reproducibility\n",
    "GLOBAL_SEED = 42\n",
    "np.random.seed(GLOBAL_SEED)\n",
    "\n",
    "# Matplotlib defaults\n",
    "plt.rcParams[\"figure.figsize\"] = (7.5, 5)\n",
    "plt.rcParams[\"axes.grid\"] = True\n",
    "sns.set_context(\"talk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d94f1d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Section 2: Configure cluster_Et Bins and Per-Bin Input Files\n",
    "# Define bin edges and derive labels\n",
    "BIN_EDGES = [5, 15, 25, 40]\n",
    "BIN_LABELS = [f\"{BIN_EDGES[i]}_{BIN_EDGES[i+1]}\" for i in range(len(BIN_EDGES)-1)]\n",
    "\n",
    "# --- Input File Configuration ---\n",
    "# Option 1: Use the same files for all bins (current setup)\n",
    "# The data loader will filter events by pT for each bin.\n",
    "# This is useful when you have inclusive data files.\n",
    "USE_SINGLE_FILE_SET = True\n",
    "\n",
    "SINGLE_FILE_SET = {\n",
    "    \"signal\": \"shapes_photon20.txt\",\n",
    "    \"background\": \"shapes_jet30.txt\"\n",
    "}\n",
    "\n",
    "# Option 2: Use different files for each pT bin.\n",
    "# This is useful if your data is already split into separate files by pT.\n",
    "# To use this, set USE_SINGLE_FILE_SET = False and edit the paths below.\n",
    "PER_BIN_FILE_SETS = {\n",
    "    \"5_15\": {\n",
    "        \"signal\": \"shapes_photon_5_15.txt\",      # Example path\n",
    "        \"background\": \"shapes_jet_5_15.txt\"     # Example path\n",
    "    },\n",
    "    \"15_25\": {\n",
    "        \"signal\": \"shapes_photon_15_25.txt\",     # Example path\n",
    "        \"background\": \"shapes_jet_15_25.txt\"    # Example path\n",
    "    },\n",
    "    \"25_40\": {\n",
    "        \"signal\": \"shapes_photon_25_40.txt\",     # Example path\n",
    "        \"background\": \"shapes_jet_25_40.txt\"    # Example path\n",
    "    },\n",
    "}\n",
    "\n",
    "# Logic to select the configuration\n",
    "if USE_SINGLE_FILE_SET:\n",
    "    PER_BIN_FILES = {bin_label: SINGLE_FILE_SET for bin_label in BIN_LABELS}\n",
    "else:\n",
    "    PER_BIN_FILES = PER_BIN_FILE_SETS\n",
    "\n",
    "\n",
    "# Column names from model_training.ipynb\n",
    "COLUMNS = [\n",
    "    \"cluster_Et\", \"cluster_Eta\", \"cluster_Phi\", \"vertexz\",\n",
    "    \"e11_over_e33\", \"e32_over_e35\", \"e11_over_e22\", \"e11_over_e13\",\n",
    "    \"e11_over_e15\", \"e11_over_e17\", \"e11_over_e31\",\n",
    "    \"e11_over_e51\", \"e11_over_e71\", \"e22_over_e33\",\n",
    "    \"e22_over_e35\", \"e22_over_e37\", \"e22_over_e53\",\n",
    "    \"cluster_prob\", \"cluster_weta_cogx\", \"cluster_wphi_cogx\",\n",
    "    \"cluster_et1\", \"cluster_et2\", \"cluster_et3\", \"cluster_et4\",\n",
    "    \"cluster_w32\", \"cluster_w52\", \"cluster_w72\", \n",
    "    \"recoisoET\", \"is_tight\", \"pid\"\n",
    "]\n",
    "\n",
    "# Key column names\n",
    "PT_COL = \"cluster_Et\"\n",
    "ISO_COL = \"recoisoET\"      # Updated from \"isoET\"\n",
    "LABEL_COL = \"label\"\n",
    "\n",
    "# Feature columns to train on from model_training.ipynb\n",
    "FEATURES = [\n",
    "    \"vertexz\",\n",
    "    \"cluster_Et\"\n",
    "    \"e11_over_e33\",\n",
    "    \"cluster_et1\",\n",
    "    \"cluster_et2\",\n",
    "    \"cluster_et3\",\n",
    "    \"cluster_et4\",\n",
    "]\n",
    "\n",
    "# Output directory for models/metrics\n",
    "OUT_DIR = Path(\"binned_models\")\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Train/val split\n",
    "TRAIN_SIZE = 0.8\n",
    "VAL_SIZE = 1 - TRAIN_SIZE\n",
    "STRATIFY = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c83635ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Section 3: Load and Concatenate Data for Each Bin\n",
    "try:\n",
    "    import uproot  # for ROOT files\n",
    "except Exception:\n",
    "    uproot = None\n",
    "\n",
    "\n",
    "def load_single(path: str, names: list) -> pd.DataFrame:\n",
    "    ext = os.path.splitext(path)[1].lower()\n",
    "    if ext in [\".csv\", \".txt\"]:\n",
    "        return pd.read_csv(path, sep=r\"\\s+\", header=0, names=names)\n",
    "    if ext in [\".pq\", \".parquet\"]:\n",
    "        return pd.read_parquet(path)\n",
    "    if ext in [\".root\"]:\n",
    "        if uproot is None:\n",
    "            raise ImportError(\"uproot is required to read ROOT files. pip install uproot\")\n",
    "        # Heuristic: read first tree and all branches\n",
    "        with uproot.open(path) as f:\n",
    "            # Pick the first TTree-like object\n",
    "            trees = [k for k in f.keys() if isinstance(f[k], uproot.behaviors.TTree.TTree)]\n",
    "            if not trees:\n",
    "                # Fall back: search members\n",
    "                trees = [k for k, v in f.items() if hasattr(v, \"arrays\")]\n",
    "            key = trees[0] if trees else list(f.keys())[0]\n",
    "            arrs = f[key].arrays(library=\"pd\")\n",
    "            return arrs\n",
    "    raise ValueError(f\"Unsupported file type: {ext}\")\n",
    "\n",
    "\n",
    "def load_bin_data(bin_label: str, et_min: float, et_max: float) -> pd.DataFrame:\n",
    "    file_map = PER_BIN_FILES.get(bin_label, {})\n",
    "    if not file_map:\n",
    "        raise FileNotFoundError(f\"No input files configured for bin {bin_label}\")\n",
    "\n",
    "    dfs = []\n",
    "    for file_type, path in file_map.items():\n",
    "        if not os.path.exists(path):\n",
    "            raise FileNotFoundError(f\"Configured file not found: {path}\")\n",
    "        df_i = load_single(path, names=COLUMNS)\n",
    "        if file_type == \"signal\":\n",
    "            df_i[LABEL_COL] = 1\n",
    "            df_i = df_i[df_i[\"pid\"].isin([1,2])] # photon only\n",
    "        elif file_type == \"background\":\n",
    "            df_i[LABEL_COL] = 0\n",
    "            df_i = df_i[~df_i[\"pid\"].isin([1,2])] # reject electrons\n",
    "        dfs.append(df_i)\n",
    "    df = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "    # Validate required columns\n",
    "    base_required = {PT_COL, ISO_COL, LABEL_COL}\n",
    "    if FEATURES:\n",
    "        required = base_required.union(FEATURES)\n",
    "    else:\n",
    "        required = base_required\n",
    "    missing = sorted(required - set(df.columns))\n",
    "    if missing:\n",
    "        raise KeyError(f\"Missing required columns for bin {bin_label}: {missing}\")\n",
    "\n",
    "    # Drop rows with missing label and key columns\n",
    "    df = df.dropna(subset=[LABEL_COL])\n",
    "\n",
    "    # Filter by Et bin\n",
    "    df = df[(df[PT_COL] >= et_min) & (df[PT_COL] < et_max)]\n",
    "\n",
    "    # Ensure numeric dtypes for key columns\n",
    "    for c in [PT_COL, ISO_COL, LABEL_COL]:\n",
    "        if c in df.columns:\n",
    "            df[c] = pd.to_numeric(df[c], errors=\"coerce\")\n",
    "\n",
    "    # Drop rows with NA on key columns after coercion\n",
    "    df = df.dropna(subset=[PT_COL, ISO_COL, LABEL_COL])\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb2be386",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Section 4: Train/Validation Split per Bin\n",
    "\n",
    "def split_train_val(df: pd.DataFrame):\n",
    "    X = df[FEATURES].copy()\n",
    "    y = df[LABEL_COL].astype(int).values\n",
    "    if STRATIFY:\n",
    "        strat = y\n",
    "    else:\n",
    "        strat = None\n",
    "    X_train, X_val, y_train, y_val = train_test_split(\n",
    "        X, y, train_size=TRAIN_SIZE, random_state=GLOBAL_SEED, stratify=strat\n",
    "    )\n",
    "    return X_train, X_val, y_train, y_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a642def",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Section 5: Build Preprocessing and Model Pipeline\n",
    "\n",
    "MODEL_CONFIG = {\n",
    "    \"classifier\": \"xgb\" if 'XGB_AVAILABLE' in globals() and XGB_AVAILABLE else \"hgb\",\n",
    "    \"params\": {\n",
    "        # Defaults for XGBClassifier; if HGB used, some keys are ignored\n",
    "        \"n_estimators\": 400,\n",
    "        \"max_depth\": 4,\n",
    "        \"learning_rate\": 0.05,\n",
    "        \"subsample\": 0.8,\n",
    "        \"colsample_bytree\": 0.8,\n",
    "        \"reg_alpha\": 0.0,\n",
    "        \"reg_lambda\": 1.0,\n",
    "        \"random_state\": GLOBAL_SEED,\n",
    "        \"n_jobs\": 4,\n",
    "        \"tree_method\": \"hist\",\n",
    "    },\n",
    "    \"use_scaler\": False,\n",
    "}\n",
    "\n",
    "\n",
    "def build_pipeline():\n",
    "    steps = []\n",
    "    steps.append((\"imputer\", SimpleImputer(strategy=\"median\")))\n",
    "    if MODEL_CONFIG.get(\"use_scaler\", False):\n",
    "        steps.append((\"scaler\", StandardScaler()))\n",
    "\n",
    "    clf_name = MODEL_CONFIG[\"classifier\"]\n",
    "    if clf_name == \"xgb\" and XGB_AVAILABLE:\n",
    "        clf = xgb.XGBClassifier(\n",
    "            **MODEL_CONFIG[\"params\"],\n",
    "            objective=\"binary:logistic\",\n",
    "            eval_metric=\"auc\",\n",
    "            use_label_encoder=False,\n",
    "        )\n",
    "    elif clf_name == \"hgb\":\n",
    "        hgb_params = {\n",
    "            \"max_depth\": None if MODEL_CONFIG[\"params\"].get(\"max_depth\", 0) <= 0 else MODEL_CONFIG[\"params\"][\"max_depth\"],\n",
    "            \"learning_rate\": MODEL_CONFIG[\"params\"].get(\"learning_rate\", 0.1),\n",
    "            \"max_iter\": MODEL_CONFIG[\"params\"].get(\"n_estimators\", 300),\n",
    "            \"l2_regularization\": MODEL_CONFIG[\"params\"].get(\"reg_lambda\", 0.0),\n",
    "            \"min_samples_leaf\": 20,\n",
    "            \"random_state\": GLOBAL_SEED,\n",
    "        }\n",
    "        clf = HistGradientBoostingClassifier(**hgb_params)\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported classifier in MODEL_CONFIG\")\n",
    "\n",
    "    steps.append((\"clf\", clf))\n",
    "    return Pipeline(steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b730511b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Section 6: Train One Model per Bin\n",
    "\n",
    "trained_pipelines = {}\n",
    "train_metrics = {}\n",
    "\n",
    "for i in range(len(BIN_EDGES) - 1):\n",
    "    et_min, et_max = BIN_EDGES[i], BIN_EDGES[i+1]\n",
    "    bin_label = BIN_LABELS[i]\n",
    "    print(f\"\\n=== Training bin {bin_label} [{et_min}, {et_max}) GeV ===\")\n",
    "\n",
    "    df_bin = load_bin_data(bin_label, et_min, et_max)\n",
    "    feats = FEATURES if FEATURES else autodetect_features(df_bin)\n",
    "    if not feats:\n",
    "        raise ValueError(f\"No FEATURES found for bin {bin_label}. Please define FEATURES explicitly.\")\n",
    "\n",
    "    X_train, X_val, y_train, y_val = split_train_val_with_features(df_bin, feats)\n",
    "\n",
    "    pipe = build_pipeline()\n",
    "    t0 = time.time()\n",
    "    pipe.fit(X_train, y_train)\n",
    "    t1 = time.time()\n",
    "\n",
    "    # Basic training AUC on train split\n",
    "    if hasattr(pipe.named_steps[\"clf\"], \"predict_proba\"):\n",
    "        y_train_proba = pipe.predict_proba(X_train)[:, 1]\n",
    "    else:\n",
    "        y_train_proba = pipe.decision_function(X_train)\n",
    "    auc_train = roc_auc_score(y_train, y_train_proba) if len(np.unique(y_train)) > 1 else np.nan\n",
    "\n",
    "    trained_pipelines[bin_label] = {\n",
    "        \"pipeline\": pipe,\n",
    "        \"features\": feats,\n",
    "        \"X_val\": X_val,\n",
    "        \"y_val\": y_val,\n",
    "        \"val_index\": X_val.index,  # store for alignment\n",
    "        \"df_bin\": df_bin,  # keep for ISO correlation reference\n",
    "    }\n",
    "    train_metrics[bin_label] = {\n",
    "        \"train_time_sec\": round(t1 - t0, 3),\n",
    "        \"n_train\": int(len(y_train)),\n",
    "        \"n_val\": int(len(y_val)),\n",
    "        \"auc_train\": float(auc_train) if not np.isnan(auc_train) else np.nan,\n",
    "        \"et_min\": float(et_min),\n",
    "        \"et_max\": float(et_max),\n",
    "    }\n",
    "\n",
    "print(\"\\nTraining complete for all bins.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bfa41ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Section 7: Persist Trained Pipelines and Metadata\n",
    "\n",
    "for bin_label in BIN_LABELS:\n",
    "    if bin_label not in trained_pipelines:\n",
    "        continue\n",
    "    entry = trained_pipelines[bin_label]\n",
    "    pipe = entry[\"pipeline\"]\n",
    "    feats = entry[\"features\"]\n",
    "\n",
    "    model_path = OUT_DIR / f\"model_{bin_label}.joblib\"\n",
    "    joblib.dump(pipe, model_path)\n",
    "\n",
    "    meta = {\n",
    "        \"bin_label\": bin_label,\n",
    "        \"bin_edges\": BIN_EDGES,\n",
    "        \"pt_col\": PT_COL,\n",
    "        \"iso_col\": ISO_COL,\n",
    "        \"label_col\": LABEL_COL,\n",
    "        \"features\": feats,\n",
    "        \"train_size\": TRAIN_SIZE,\n",
    "        \"random_seed\": GLOBAL_SEED,\n",
    "        \"model_config\": MODEL_CONFIG,\n",
    "        \"train_metrics\": train_metrics.get(bin_label, {}),\n",
    "        \"input_files\": PER_BIN_FILES.get(bin_label, []),\n",
    "    }\n",
    "    with open(OUT_DIR / f\"metadata_{bin_label}.json\", \"w\") as f:\n",
    "        json.dump(meta, f, indent=2)\n",
    "\n",
    "print(f\"Saved models and metadata to: {OUT_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65fc0fea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Section 8: Evaluate AUC per pT/cluster_Et Bin\n",
    "\n",
    "val_results = {}\n",
    "\n",
    "for bin_label in BIN_LABELS:\n",
    "    if bin_label not in trained_pipelines:\n",
    "        continue\n",
    "    entry = trained_pipelines[bin_label]\n",
    "    pipe = entry[\"pipeline\"]\n",
    "    X_val, y_val = entry[\"X_val\"], entry[\"y_val\"]\n",
    "\n",
    "    if hasattr(pipe.named_steps[\"clf\"], \"predict_proba\"):\n",
    "        y_proba = pipe.predict_proba(X_val)[:, 1]\n",
    "    else:\n",
    "        y_proba = pipe.decision_function(X_val)\n",
    "\n",
    "    auc = roc_auc_score(y_val, y_proba) if len(np.unique(y_val)) > 1 else np.nan\n",
    "    fpr, tpr, thr = roc_curve(y_val, y_proba) if len(np.unique(y_val)) > 1 else (None, None, None)\n",
    "\n",
    "    val_results[bin_label] = {\n",
    "        \"auc_val\": float(auc) if not np.isnan(auc) else np.nan,\n",
    "        \"fpr\": fpr,\n",
    "        \"tpr\": tpr,\n",
    "        \"thresholds\": thr,\n",
    "        \"n_val\": int(len(y_val)),\n",
    "    }\n",
    "\n",
    "val_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbb51c51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Section 9: Compute isoET vs BDT Score Correlation per Bin\n",
    "\n",
    "correlation_results = {}\n",
    "\n",
    "for bin_label in BIN_LABELS:\n",
    "    if bin_label not in trained_pipelines:\n",
    "        continue\n",
    "    entry = trained_pipelines[bin_label]\n",
    "    pipe = entry[\"pipeline\"]\n",
    "    X_val, y_val, df_bin, val_index = entry[\"X_val\"], entry[\"y_val\"], entry[\"df_bin\"], entry[\"val_index\"]\n",
    "\n",
    "    # isoET values for the validation rows\n",
    "    iso_val = df_bin.loc[val_index, ISO_COL].to_numpy()\n",
    "\n",
    "    # Predicted prob on validation set\n",
    "    if hasattr(pipe.named_steps[\"clf\"], \"predict_proba\"):\n",
    "        y_proba = pipe.predict_proba(X_val)[:, 1]\n",
    "    else:\n",
    "        y_proba = pipe.decision_function(X_val)\n",
    "\n",
    "    # Drop any NaNs\n",
    "    mask = np.isfinite(iso_val) & np.isfinite(y_proba)\n",
    "    iso_c = iso_val[mask]\n",
    "    proba_c = y_proba[mask]\n",
    "\n",
    "    pearson_r, pearson_p = stats.pearsonr(iso_c, proba_c) if len(iso_c) > 1 else (np.nan, np.nan)\n",
    "    spearman_rho, spearman_p = stats.spearmanr(iso_c, proba_c) if len(iso_c) > 1 else (np.nan, np.nan)\n",
    "\n",
    "    correlation_results[bin_label] = {\n",
    "        \"pearson_r\": float(pearson_r) if np.isfinite(pearson_r) else np.nan,\n",
    "        \"pearson_p\": float(pearson_p) if np.isfinite(pearson_p) else np.nan,\n",
    "        \"spearman_rho\": float(spearman_rho) if np.isfinite(spearman_rho) else np.nan,\n",
    "        \"spearman_p\": float(spearman_p) if np.isfinite(spearman_p) else np.nan,\n",
    "        \"n_pairs\": int(np.sum(mask)),\n",
    "    }\n",
    "\n",
    "correlation_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "820334a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Section 10: Visualize AUC and isoET–BDT Relationships\n",
    "\n",
    "# AUC bar plot\n",
    "auc_data = [(b, val_results[b][\"auc_val\"]) for b in BIN_LABELS if b in val_results]\n",
    "if auc_data:\n",
    "    bins_, aucs_ = zip(*auc_data)\n",
    "    plt.figure()\n",
    "    sns.barplot(x=list(bins_), y=list(aucs_), color=\"steelblue\")\n",
    "    plt.ylabel(\"Validation AUC\")\n",
    "    plt.xlabel(\"cluster_Et bin [GeV]\")\n",
    "    plt.title(\"AUC by pT bin\")\n",
    "    plt.ylim(0.5, 1.0)\n",
    "    for i, v in enumerate(aucs_):\n",
    "        plt.text(i, v + 0.01, f\"{v:.3f}\", ha=\"center\")\n",
    "    plt.show()\n",
    "\n",
    "# isoET vs BDT score scatter/hexbin per bin\n",
    "for bin_label in BIN_LABELS:\n",
    "    if bin_label not in trained_pipelines:\n",
    "        continue\n",
    "    entry = trained_pipelines[bin_label]\n",
    "    pipe = entry[\"pipeline\"]\n",
    "    X_val = entry[\"X_val\"]\n",
    "    df_bin = entry[\"df_bin\"]\n",
    "    val_index = entry[\"val_index\"]\n",
    "\n",
    "    iso_val = df_bin.loc[val_index, ISO_COL].to_numpy()\n",
    "\n",
    "    if hasattr(pipe.named_steps[\"clf\"], \"predict_proba\"):\n",
    "        y_proba = pipe.predict_proba(X_val)[:, 1]\n",
    "    else:\n",
    "        y_proba = pipe.decision_function(X_val)\n",
    "\n",
    "    mask = np.isfinite(iso_val) & np.isfinite(y_proba)\n",
    "    iso_c, proba_c = iso_val[mask], y_proba[mask]\n",
    "\n",
    "    plt.figure()\n",
    "    if len(iso_c) > 5000:\n",
    "        plt.hexbin(iso_c, proba_c, gridsize=40, cmap=\"viridis\", mincnt=1)\n",
    "        cb = plt.colorbar()\n",
    "        cb.set_label(\"count\")\n",
    "    else:\n",
    "        plt.scatter(iso_c, proba_c, s=8, alpha=0.4)\n",
    "    plt.xlabel(\"isoET\")\n",
    "    plt.ylabel(\"BDT score (prob)\")\n",
    "    plt.title(f\"isoET vs BDT score — bin {bin_label}\")\n",
    "    plt.show()\n",
    "\n",
    "# Optional: ROC curves per bin\n",
    "for bin_label in BIN_LABELS:\n",
    "    if bin_label not in val_results:\n",
    "        continue\n",
    "    r = val_results[bin_label]\n",
    "    if r[\"fpr\"] is None:\n",
    "        continue\n",
    "    plt.figure()\n",
    "    plt.plot(r[\"fpr\"], r[\"tpr\"], label=f\"{bin_label} (AUC={r['auc_val']:.3f})\")\n",
    "    plt.plot([0,1],[0,1],\"k--\", alpha=0.5)\n",
    "    plt.xlabel(\"FPR\")\n",
    "    plt.ylabel(\"TPR\")\n",
    "    plt.title(\"ROC per bin\")\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64ae9db7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Section 11: Summarize and Save Metrics Table\n",
    "\n",
    "rows = []\n",
    "for bin_label in BIN_LABELS:\n",
    "    row = {\"bin\": bin_label}\n",
    "    # Training metrics\n",
    "    row.update(train_metrics.get(bin_label, {}))\n",
    "    # Validation AUC\n",
    "    if bin_label in val_results:\n",
    "        row[\"auc_val\"] = val_results[bin_label][\"auc_val\"]\n",
    "        row[\"n_val\"] = val_results[bin_label][\"n_val\"]\n",
    "    else:\n",
    "        row[\"auc_val\"] = np.nan\n",
    "        row[\"n_val\"] = 0\n",
    "    # Correlations\n",
    "    if bin_label in correlation_results:\n",
    "        row.update(correlation_results[bin_label])\n",
    "    else:\n",
    "        row.update({\n",
    "            \"pearson_r\": np.nan,\n",
    "            \"pearson_p\": np.nan,\n",
    "            \"spearman_rho\": np.nan,\n",
    "            \"spearman_p\": np.nan,\n",
    "            \"n_pairs\": 0,\n",
    "        })\n",
    "    rows.append(row)\n",
    "\n",
    "metrics_df = pd.DataFrame(rows)\n",
    "metrics_df = metrics_df[[\n",
    "    \"bin\", \"et_min\", \"et_max\", \"n_train\", \"n_val\", \"auc_train\", \"auc_val\",\n",
    "    \"pearson_r\", \"pearson_p\", \"spearman_rho\", \"spearman_p\", \"n_pairs\", \"train_time_sec\"\n",
    "]].sort_values(\"bin\")\n",
    "\n",
    "metrics_path = OUT_DIR / \"per_bin_metrics.csv\"\n",
    "metrics_df.to_csv(metrics_path, index=False)\n",
    "\n",
    "print(\"Metrics saved:\", metrics_path)\n",
    "metrics_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "663346c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Section 3.1: Auto-detect FEATURES if empty (numeric columns excluding PT/ISO/LABEL)\n",
    "\n",
    "def autodetect_features(df: pd.DataFrame) -> list:\n",
    "    numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "    exclude = {PT_COL, ISO_COL, LABEL_COL}\n",
    "    feats = [c for c in numeric_cols if c not in exclude]\n",
    "    return feats\n",
    "\n",
    "\n",
    "def split_train_val_with_features(df: pd.DataFrame, features: list):\n",
    "    X = df[features].copy()\n",
    "    y = df[LABEL_COL].astype(int).values\n",
    "    strat = y if STRATIFY else None\n",
    "    X_train, X_val, y_train, y_val = train_test_split(\n",
    "        X, y, train_size=TRAIN_SIZE, random_state=GLOBAL_SEED, stratify=strat\n",
    "    )\n",
    "    return X_train, X_val, y_train, y_val"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
